{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4bd2a7-3609-42f9-9cae-bc60c9f98b6c",
   "metadata": {},
   "source": [
    "# NLP Disaster Tweets Continued\n",
    "## Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e96d7d0-4744-48db-ad93-0ae2276a1cb6",
   "metadata": {},
   "source": [
    "### Import preprocessed data\n",
    "I imported preprocessed data using the same preprocessing steps from HW2. A summary of the preprocessing steps is shown below.\n",
    "\n",
    "1. Lowercase all letters\n",
    "1. Only keep alphanumerics, # and @\n",
    "1. Keep the http or https part of links, get rid of the rest of the link\n",
    "1. Keep the @ part of @usernames, get rid of the actual username\n",
    "1. Keep the # of #topics, keep both topic and # but separate them\n",
    "1. Lemmatize all words\n",
    "1. Remove common stop words (and, or, the ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf8e37b-0298-42c0-a511-f26466bf2c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10691</td>\n",
       "      <td>wreck</td>\n",
       "      <td>?currently writing a book?</td>\n",
       "      <td>friggin wreck destiel suck read vine descripti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7148</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>Malibu/SantaFe/Winning!</td>\n",
       "      <td>sterlingscott red carpet fundrais oso mudslid ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>casualties</td>\n",
       "      <td>Canadian bread</td>\n",
       "      <td>@ honest peopl want go rampag let use hand fee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>262</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>http twelv fear kill pakistani air ambul helic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4468</td>\n",
       "      <td>electrocuted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got electrocut last night work first time life...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4176</td>\n",
       "      <td>drown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>older nativ australian believ ocean creat urin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10243</td>\n",
       "      <td>volcano</td>\n",
       "      <td>West Coast, Cali USA</td>\n",
       "      <td>architect behind kany west volcano http</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>681</td>\n",
       "      <td>attack</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>india shud give ani evid pakthey share terrori...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1399</td>\n",
       "      <td>body%20bag</td>\n",
       "      <td>New York</td>\n",
       "      <td>auth loui vuitton brown saumur 35 cross bodi s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>296</td>\n",
       "      <td>annihilated</td>\n",
       "      <td>Higher Places</td>\n",
       "      <td>episod trunk annihil freiza cleanest shit ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       keyword                     location  \\\n",
       "10  10691         wreck   ?currently writing a book?   \n",
       "11   7148      mudslide      Malibu/SantaFe/Winning!   \n",
       "12   2017    casualties               Canadian bread   \n",
       "13    262     ambulance                    Amsterdam   \n",
       "14   4468  electrocuted                          NaN   \n",
       "15   4176         drown                          NaN   \n",
       "16  10243       volcano         West Coast, Cali USA   \n",
       "17    681        attack                       Mumbai   \n",
       "18   1399    body%20bag                     New York   \n",
       "19    296   annihilated                Higher Places   \n",
       "\n",
       "                                                 text  target  \n",
       "10  friggin wreck destiel suck read vine descripti...       0  \n",
       "11  sterlingscott red carpet fundrais oso mudslid ...       1  \n",
       "12  @ honest peopl want go rampag let use hand fee...       0  \n",
       "13  http twelv fear kill pakistani air ambul helic...       1  \n",
       "14  got electrocut last night work first time life...       0  \n",
       "15  older nativ australian believ ocean creat urin...       0  \n",
       "16            architect behind kany west volcano http       0  \n",
       "17  india shud give ani evid pakthey share terrori...       1  \n",
       "18  auth loui vuitton brown saumur 35 cross bodi s...       0  \n",
       "19  episod trunk annihil freiza cleanest shit ever...       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"/home/jiwonjjeong/cornell-tech/CT-applied-machine-learning/hw-2/data-from-NLP-disaster-tweets/preprocessed/train.csv\")\n",
    "test = pd.read_csv(\"/home/jiwonjjeong/cornell-tech/CT-applied-machine-learning/hw-2/data-from-NLP-disaster-tweets/preprocessed/test.csv\")\n",
    "val = pd.read_csv(\"/home/jiwonjjeong/cornell-tech/CT-applied-machine-learning/hw-2/data-from-NLP-disaster-tweets/preprocessed/validate.csv\")\n",
    "print(train.shape)\n",
    "train[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c6df8-e9a5-44b7-a573-0e28e11ed410",
   "metadata": {},
   "source": [
    "### Create feature vectors using Naive-Bayes assumption\n",
    "I will assume that each word can be considered independently, allowing me to represent the inclusion of each word as its own individual feature. Below, I am creating the word dictionary to use as features based on the training set words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a99f0b-ff9f-4ac2-a71d-3207fe171eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get set of unique words in the training set\n",
    "unique_words = set(' '.join(train['text']).split())\n",
    "len(unique_words)\n",
    "words = list(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c4afa68-3632-426f-ab9d-1890118fe0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10339 dimensional vector to represent each sample and embedd using naive bayes assumption\n",
    "train_dict = {}\n",
    "val_dict = {}\n",
    "test_dict = {}\n",
    "for word in words:\n",
    "    train_dict[f'has_{word}'] = train['text'].str.contains(word, na=False).astype(int)\n",
    "    val_dict[f'has_{word}'] = val['text'].str.contains(word, na=False).astype(int)\n",
    "    test_dict[f'has_{word}'] = test['text'].str.contains(word, na=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9285278-748e-4187-85bb-e58183807656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_wasnt</th>\n",
       "      <th>has_cree</th>\n",
       "      <th>has_rescueadoptionloc</th>\n",
       "      <th>has_tcop5zicjudxo</th>\n",
       "      <th>has_presley</th>\n",
       "      <th>has_sel</th>\n",
       "      <th>has_damnwa</th>\n",
       "      <th>has_hitter</th>\n",
       "      <th>has_humanitarian</th>\n",
       "      <th>has_antonio</th>\n",
       "      <th>...</th>\n",
       "      <th>has_stuffin</th>\n",
       "      <th>has_declin</th>\n",
       "      <th>has_vermilion</th>\n",
       "      <th>has_pjnet</th>\n",
       "      <th>has_convert</th>\n",
       "      <th>has_cfc</th>\n",
       "      <th>has_homeland</th>\n",
       "      <th>has_nh</th>\n",
       "      <th>has_bruv</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10340 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   has_wasnt  has_cree  has_rescueadoptionloc  has_tcop5zicjudxo  has_presley  \\\n",
       "0          0         0                      0                  0            0   \n",
       "1          0         0                      0                  0            0   \n",
       "2          0         0                      0                  0            0   \n",
       "3          0         0                      0                  0            0   \n",
       "4          0         0                      0                  0            0   \n",
       "\n",
       "   has_sel  has_damnwa  has_hitter  has_humanitarian  has_antonio  ...  \\\n",
       "0        0           0           0                 0            0  ...   \n",
       "1        0           0           0                 0            0  ...   \n",
       "2        0           0           0                 0            0  ...   \n",
       "3        0           0           0                 0            0  ...   \n",
       "4        0           0           0                 0            0  ...   \n",
       "\n",
       "   has_stuffin  has_declin  has_vermilion  has_pjnet  has_convert  has_cfc  \\\n",
       "0            0           0              0          0            0        0   \n",
       "1            0           0              0          0            0        0   \n",
       "2            0           0              0          0            0        0   \n",
       "3            0           0              0          0            0        0   \n",
       "4            0           0              0          0            0        0   \n",
       "\n",
       "   has_homeland  has_nh  has_bruv  target  \n",
       "0             0       0         0       0  \n",
       "1             0       0         0       1  \n",
       "2             0       0         0       1  \n",
       "3             0       0         0       1  \n",
       "4             0       0         0       0  \n",
       "\n",
       "[5 rows x 10340 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the dictionary into dataframe\n",
    "train_no_target = pd.DataFrame(train_dict)\n",
    "val_no_target = pd.DataFrame(val_dict)\n",
    "test = pd.DataFrame(test_dict)\n",
    "train = pd.concat([train_no_target,train[\"target\"]], axis=1)\n",
    "val = pd.concat([val_no_target,train[\"target\"]], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c079852-9238-4086-9aa3-6949e2a79838",
   "metadata": {},
   "source": [
    "### Compute model parameters\n",
    "I used the closed-form solution for the parameters for a Bernoulli Naive Bayes model. This results in an analytical solution for the parameters psis which helps calculate P(x|y) and parameters phis which helps calculate P(y).\n",
    "\n",
    "I also used Laplace smoothing to make sure that words outside of the training/model vocabulary will give a P(x|y) probability of 0.5 (no bias towards either 0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bac54368-47ea-456e-a195-847e27d81729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56746106 0.43253894]\n"
     ]
    }
   ],
   "source": [
    "# Compute model parameters\n",
    "# Code adapted from lecture notes\n",
    "import numpy as np\n",
    "alpha = 1\n",
    "n = train.shape[0]\n",
    "d = train.shape[1] - 1\n",
    "K = 2\n",
    "\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "y_train = train[\"target\"]\n",
    "X_train = train.drop(columns=[\"target\"])\n",
    "\n",
    "for k in range(K):\n",
    "    X_k = X_train[y_train == k]\n",
    "    psis[k] = (X_k.sum(axis=0) + alpha)/ (2*alpha + X_k.shape[0])\n",
    "    phis[k] = X_k.shape[0]/ float(n)\n",
    "\n",
    "print(phis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b5405-fbe2-458c-9dad-e25d3bd5ece6",
   "metadata": {},
   "source": [
    "### Compute predictions from model\n",
    "Using the modeled distribution of inputs for a given class P(x|y=k) and the modeled distribution of the class P(y=k), I can compute the predicted P(y=k|x) for each class k by multiplying these two values. To avoid numerical instability, I added the logs of these probabilities.\n",
    "\n",
    "Then, I determined that the predicted class for a given input should be the class that has the highest probability for that input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d0f86d-dbec-49e3-bff5-c72be69f2045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Compute predicitons from model\n",
    "# Code adapted from lecture notes\n",
    "# And code adapted from ChatGPT (2025)\n",
    "# Prompt: \"how can i make sure i dont crash due to memory in this funciton...\"\n",
    "def nb_predictions(x, psis, phis):\n",
    "    x = x.to_numpy()\n",
    "\n",
    "    n, d = x.shape\n",
    "    K = psis.shape[0]\n",
    "\n",
    "    logpy = np.log(phis).reshape([K])\n",
    "    logpyx = np.zeros((K, n))\n",
    "\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    log_psis = np.log(psis)\n",
    "    log_one_minus_psis = np.log(1-psis)\n",
    "\n",
    "    for i in range(n):\n",
    "        xi = x[i]  # shape (d,)\n",
    "        logpxy = xi * log_psis + (1-xi) * log_one_minus_psis  # shape (K,d)\n",
    "        logpyx[:, i] = logpxy.sum(axis=1) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0), logpyx\n",
    "\n",
    "idx, logpyx = nb_predictions(X_train, psis, phis)\n",
    "print(idx[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3656c0-4e1d-480d-8c09-2a3f59520314",
   "metadata": {},
   "source": [
    "## Compare models\n",
    "I applied the model to calculate the predicted class of the validation/development set. Then I used the F1 score to evaluate the classification model's precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ddcaf3-a4b2-4780-b1e8-cfdef43d6e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Apply to validation set\n",
    "X_val = val.drop(columns=[\"target\"])\n",
    "idx_val, logpyx = nb_predictions(X_val, psis, phis)\n",
    "print(idx_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94c1d86-dd04-4a75-9b03-f8d23870c4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.2289348171701113\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(idx_val, val[\"target\"])\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c57a4816-1b6f-4a21-964c-ccd6827c2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models to HW2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c44ec-edf3-49b3-ad0d-c98b619fea19",
   "metadata": {},
   "source": [
    "Recall, the best model between the N-gram and bag of words approach had an F1 score of 0.743139407244786. The best model of overall was with the M=3 feature set using L2 regularization with C=1 with the bag of words model. \n",
    "\n",
    "When the model was rebuilt using the entire training data and submitted to Kaggle, the F1 score rose to 0.7537796976241901.\n",
    "\n",
    "By comparison, the Naive-Bayes approach had an F1 score of 0.2289348171701113. Thus, it seems that the Naive-Bayes approach is significantly worse at generalizing and predicting if tweets are disasters on new data.\n",
    "\n",
    "I found this low score so surprising that I decided to check if my model was overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7da2a7f3-459a-4280-a665-a3d8ac61cb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing F1 score: 0.8132992327365729\n"
     ]
    }
   ],
   "source": [
    "# See F1 score for testing data\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(idx, train[\"target\"])\n",
    "print(\"Testing F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9dfbe3-9f43-43f3-86c5-3f1a237eb7f6",
   "metadata": {},
   "source": [
    "Because the testing F1 score is very high while validation F1 score is low, it is observed that my NB model is overfitting on the testing data. I think one reason is for this is that I included all words in the vocabulary as features, even if it only appears once in the training data. This is likely created too many features compared to samples and induced overfitting on these poorly represented single-occurence words.\n",
    "\n",
    "A better alternative is to only take the top 1000 highest frequency words, which is better done through the sklearn CountVectorizer library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351200c-42da-474a-8217-eb2953b41c3a",
   "metadata": {},
   "source": [
    "### Comparison of generative and discriminative models\n",
    "The previous bag of words and N-gram models are discriminative models, where they explicitly determine P(y|x) by modeling the data generative function between the input x and label y. This Bernoulli Naive-Bayes model is a generative model, where it models the distribution of inputs for each label. It is still able to perform discriminative-like predictions using the Bayes Rule (this is different from the NB assumption).\n",
    "\n",
    "One positive of generative models is that it can perform discriminative-model-like predictions while also performing its task to generate new inputs with high probability of class k. Discriminative models do not have the capability to generate new inputs. \n",
    "\n",
    "An additional positive is that generative models are more descriptive of the dataset. Discriminative models only classify classes of datasets by determining decision boundaries between classes. On the other hand, generative models show the distributions of the data. This means it can capture clustering of data within classes, showing their means and variances. This information about the distribution can help with more tasks like data imputation, handling missing data, and anomaly detection. Discriminative models do not have this additional capability.\n",
    "\n",
    "On a performance level, discriminative models do better when there is more data close to the true decision boundary. So the difference in predictive power of a discriminative vs generative supervised learning model will depend on the dataset and underlying decision boundaries. For example, with less data, generative models will perform better.\n",
    "\n",
    "On a negative for generative models, they require stronger assumptions to realistically learn distributions. Thus, there is a higher risk of inaccuracies coming for inappropriate, strong assumptions. The comparison of assumptions will be discussed in the next section.\n",
    "\n",
    "Additionally, discriminative models are easier to implement and understand on an interpretative level for the task of classification prediction. We can simply look at the model parameter magnitudes to determine the most important and least important features with a discrinative model. By comparison, a generative model requires more steps (Bayes Rule) to perform predictive classification tasks. These complexities become harder to track and compare as we grow from a binary classification to a multiclassification task. By comparison, comparing magnitudes of parameters in a discrimnative classification is more straight-forward.\n",
    "\n",
    "### Asssumptions of generative and discriminative models\n",
    "The Naive Bayes assumption is that the features for a given label can be considered independent from eachother. In this case, the NB assumption manifests as the presence of some specific word for non-disaster tweets like \"happy\" is independent from the word \"help\".\n",
    "\n",
    "There is also an assumption of independence in disciminative models. Here, independence means that two different features each contribute an independent effect on the decision to classify a tweet as disaster or not. While this sounds very similar, in the N-gram and bag of words model, we were able to construct our features the a unique collection of words. For example, a vocabulary of 3 words would cause features to have the form of (0,0,0), (0,0,1), (0,1,0), (0,1,1),... (1,1,1). This means that there is no assumption of individual words having independent effect on the classification from eachother.\n",
    "\n",
    "In terms of efficiency, the NB assumption is definitely important and very efficient for memory. In the example above, we saw that a vocabulary of 3 requires 9 different features for each combination of the word being present or absent. By using the NB assumption, we can represent the same vocabulary of 3 words with only 3 features. With each feature being whether each word is present or not, independently. In a general form, the NB assumption allows reduction of parameters from 2^(d-1) to ~Kd, where d is the vocabulary size and K is the number of classes. Using the NB assumption allows the inclusion of more vocabulary while maintaining memory constraints.\n",
    "\n",
    "In terms of validity, there is some appropriateness. For most unrelated words like \"girl\" and \"computer\", assuming independence of these word occurences is valid. However, we can also construct cases where words together create different meanings. For example, \"not\" and \"happy\" is very different from the independent occurence of \"not\" and \"happy\". Some words even change semantic meaning in a dependent way to another word. For example \"bank\" means different things when paired with \"river\" versus \"Chase\" (thus showing dependence between words). In general, independence is a poor assumption in natural language text since many words have context around other words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
